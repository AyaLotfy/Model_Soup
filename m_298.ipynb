{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9GQR-J4ARAL",
        "outputId": "cae607fa-7b05-4721-814f-40d34a139738"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "results_h = 'results_h.jsonl'\n",
        "results_t = 'results_t.jsonl'\n",
        "best_model_f = 'best_model_f.jsonl'\n"
      ],
      "metadata": {
        "id": "HyWBNiBxMvx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKZ0yTrMawyy"
      },
      "outputs": [],
      "source": [
        "!mkdir ckpts\n",
        "!mkdir reports\n",
        "!mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mck7F4eKN6iA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cabab0d-ed3a-4f35-d772-c39bf7ac8de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.23.5)\n",
            "Collecting boto3 (from pytorch_pretrained_bert)\n",
            "  Downloading boto3-1.28.61-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2023.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (17.0.2)\n",
            "Collecting botocore<1.32.0,>=1.31.61 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading botocore-1.31.61-py3-none-any.whl (11.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3->pytorch_pretrained_bert)\n",
            "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.61->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->pytorch_pretrained_bert)\n",
            "  Downloading urllib3-1.26.17-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.61->boto3->pytorch_pretrained_bert) (1.16.0)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=27a032cfce5d15f8e2a7b803e486493bbb8b4b47e421c1e2999b8e69834a81c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil, urllib3, safetensors, jmespath, botocore, s3transfer, huggingface-hub, tokenizers, boto3, transformers, pytorch_pretrained_bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.6\n",
            "    Uninstalling urllib3-2.0.6:\n",
            "      Successfully uninstalled urllib3-2.0.6\n",
            "Successfully installed GPUtil-1.4.0 boto3-1.28.61 botocore-1.31.61 huggingface-hub-0.17.3 jmespath-1.0.1 pytorch_pretrained_bert-0.6.2 s3transfer-0.7.0 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0 urllib3-1.26.17\n",
            "Collecting barbar\n",
            "  Downloading barbar-0.2.1-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: barbar\n",
            "Successfully installed barbar-0.2.1\n",
            "\u001b[33mWARNING: Skipping emoji as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting emoji==1.7\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=cfba8a3fa3755bb5feb6382bc0e976c68bdebe48f4ffaaa8e331519ac457606b\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n",
            "Collecting emojis\n",
            "  Downloading emojis-0.7.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: emojis\n",
            "Successfully installed emojis-0.7.0\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (1.26.17)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-2.0.6-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.17\n",
            "    Uninstalling urllib3-1.26.17:\n",
            "      Successfully uninstalled urllib3-1.26.17\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "botocore 1.31.61 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.0.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed urllib3-2.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install GPUtil pytorch_pretrained_bert transformers\n",
        "!pip install barbar\n",
        "!pip uninstall emoji\n",
        "!pip install emoji==1.7\n",
        "!pip install emojis\n",
        "!pip install --upgrade urllib3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz5sqG5EKhsw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from barbar import Bar\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import re\n",
        "from emoji import UNICODE_EMOJI\n",
        "import sys\n",
        "import emojis\n",
        "import numpy\n",
        "import shutil\n",
        "\n",
        "from statistics import mean\n",
        "import math\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nu1LiPIrsUt"
      },
      "outputs": [],
      "source": [
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
        "        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / np.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "\n",
        "        # Inputs:\n",
        "        #       input: of shape (batch_size, input_size)\n",
        "        #       hx: of shape (batch_size, hidden_size)\n",
        "        # Output:\n",
        "        #       hy: of shape (batch_size, hidden_size)\n",
        "\n",
        "        if hx is None:\n",
        "            hx = Variable(input.new_zeros(input.size(0), self.hidden_size))\n",
        "\n",
        "        x_t = self.x2h(input)\n",
        "        h_t = self.h2h(hx)\n",
        "\n",
        "\n",
        "        x_reset, x_upd, x_new = x_t.chunk(3, 1)\n",
        "        h_reset, h_upd, h_new = h_t.chunk(3, 1)\n",
        "\n",
        "        reset_gate = torch.sigmoid(x_reset + h_reset)\n",
        "        update_gate = torch.sigmoid(x_upd + h_upd)\n",
        "        new_gate = torch.tanh(x_new + (reset_gate * h_new))\n",
        "\n",
        "        hy = update_gate * hx + (1 - update_gate) * new_gate\n",
        "\n",
        "        return hy\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.rnn_cell_list = nn.ModuleList()\n",
        "\n",
        "        self.rnn_cell_list.append(GRUCell(self.input_size,\n",
        "                                          self.hidden_size,\n",
        "                                          self.bias))\n",
        "        for l in range(1, self.num_layers):\n",
        "            self.rnn_cell_list.append(GRUCell(self.hidden_size,\n",
        "                                              self.hidden_size,\n",
        "                                              self.bias))\n",
        "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "\n",
        "        # Input of shape (batch_size, seqence length, input_size)\n",
        "        #\n",
        "        # Output of shape (batch_size, output_size)\n",
        "\n",
        "        if hx is None:\n",
        "            if torch.cuda.is_available():\n",
        "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size).cuda())\n",
        "            else:\n",
        "                h0 = Variable(torch.zeros(self.num_layers, input.size(0), self.hidden_size))\n",
        "\n",
        "        else:\n",
        "             h0 = hx\n",
        "\n",
        "        outs = []\n",
        "\n",
        "        hidden = list()\n",
        "        for layer in range(self.num_layers):\n",
        "            hidden.append(h0[layer, :, :])\n",
        "\n",
        "        for t in range(input.size(1)):\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "\n",
        "                if layer == 0:\n",
        "                    hidden_l = self.rnn_cell_list[layer](input[:, t, :], hidden[layer])\n",
        "                else:\n",
        "                    hidden_l = self.rnn_cell_list[layer](hidden[layer - 1],hidden[layer])\n",
        "                hidden[layer] = hidden_l\n",
        "\n",
        "                hidden[layer] = hidden_l\n",
        "\n",
        "            outs.append(hidden_l)\n",
        "\n",
        "        # Take only last time step. Modify for seq to seq\n",
        "        out = outs[-1].squeeze()\n",
        "\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15fHQ5Kyc0ON"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FADsPUQyOTQD"
      },
      "outputs": [],
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, df, pretraine_path='xlm-roberta-base', max_length=128):\n",
        "        self.df = df\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretraine_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.df.iloc[index]['tweet']\n",
        "        label = self.df.iloc[index][\"sarcastic\"]\n",
        "\n",
        "        encoded_input = self.tokenizer(\n",
        "                text,\n",
        "                max_length = self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        input_ids = encoded_input[\"input_ids\"]\n",
        "        attention_mask = encoded_input[\"attention_mask\"] if \"attention_mask\" in encoded_input else None\n",
        "\n",
        "        data_input = {\n",
        "            \"input_ids\":input_ids.flatten(),\n",
        "            \"attention_mask\": attention_mask.flatten()\n",
        "        }\n",
        "\n",
        "        label_input ={\n",
        "            \"sarcasm\": torch.tensor(label, dtype=torch.float),\n",
        "        }\n",
        "\n",
        "        return data_input, label_input\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ob4YoaMNLq8"
      },
      "outputs": [],
      "source": [
        "def label_rule(mis_preds, cat_preds):\n",
        "    for i in range(len(mis_preds)):\n",
        "        if mis_preds[i] == 0:\n",
        "            cat_preds[i] = 0\n",
        "    return cat_preds\n",
        "\n",
        "\n",
        "def accuracy(preds, y):\n",
        "    all_output = preds.float().cpu()\n",
        "    all_label = y.float().cpu()\n",
        "    _, predict = torch.max(all_output, 1)\n",
        "    acc = accuracy_score(all_label.numpy(), torch.squeeze(predict).float().numpy())\n",
        "    return acc\n",
        "\n",
        "def calc_accuracy(preds,y):\n",
        "    predict = torch.argmax(preds, dim=1)\n",
        "    accuracy = torch.sum(predict == y.squeeze()).float().item()\n",
        "    return accuracy / float(preds.size()[0])\n",
        "\n",
        "def calc_f1_sarcasm(preds,y):\n",
        "\n",
        "    predict = torch.argmax(preds, dim=1)\n",
        "    f1_sarcastic = f1_score(y, predict, average='binary', pos_label=1)\n",
        "    return f1_sarcastic\n",
        "\n",
        "def calc_f1_score(preds,y):\n",
        "    predict = torch.argmax(preds, dim=1)\n",
        "    f1_score_macro = f1_score(y, predict, average='macro')\n",
        "    return f1_score_macro\n",
        "\n",
        "def binary_accuracy2(preds, y):\n",
        "    # round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds).squeeze()\n",
        "\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / (y.size(0))\n",
        "    return acc\n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "    # round predictions to the closest integer\n",
        "    rounded_preds = torch.round(preds).squeeze()\n",
        "\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / (y.size(0) * y.size(1))\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnUk1BOZnkat"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiCkdKaannS3"
      },
      "outputs": [],
      "source": [
        "\n",
        "dic = {\n",
        "      \"egypt\": 'المصرية',\n",
        "\t  \"nile\": 'المصرية',\n",
        "\t  \"msa\": \"اللغة العربية الفصحى\",\n",
        "\t  \"magreb\": \"المغربية\",\n",
        "\t  \"gulf\": \"الخليجية\",\n",
        "\t  \"levant\": \"الشامية\"\n",
        "}\n",
        "\n",
        "def is_emoji(s):\n",
        "    return s in UNICODE_EMOJI\n",
        "\n",
        "# add space near your emoji\n",
        "def add_space(text):\n",
        "    return ''.join(' ' + char if is_emoji(char) else char for char in text).strip()\n",
        "\n",
        "def preprocess(text):\n",
        "    sent = add_space(text)\n",
        "    sent = re.sub(r'(?:@[\\w_]+)', \"user\", sent)\n",
        "    sent = re.sub(r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', \"url\", sent)\n",
        "    sent = sent.replace('_', ' ')\n",
        "    sent = sent.replace('#', ' ')\n",
        "    return sent\n",
        "\n",
        "def prepare_text(df, col):\n",
        "    if col == 'tweet':\n",
        "        df['dialect'] = df['dialect'].map(dic)\n",
        "    for i in range(df.shape[0]):\n",
        "        df.loc[i, col] = df.loc[i, 'dialect'] + ' [SEP] ' + df.loc[i, col]\n",
        "\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTrainValData2(size=0.2, batchsize=16, num_worker=0, pretraine_path=\"marbert\", seed=42, max_length=128):\n",
        "    path = \"/content/drive/MyDrive/iSarcasm/Datasets2/split_80_20/train.csv\"\n",
        "    data = pd.read_csv(path, encoding='utf-8')\n",
        "    data = data[~data.tweet.isna()]\n",
        "    data['tweet'] = data['tweet'].apply(lambda x: preprocess(x))\n",
        "    data = prepare_text(data, col='tweet')\n",
        "\n",
        "    rephrase_df = data[[\"rephrase\", \"dialect\"]]\n",
        "    rephrase_df = rephrase_df.dropna().reset_index(drop=True)\n",
        "    rephrase_df['rephrase'] = rephrase_df['rephrase'].apply(lambda x: preprocess(x))\n",
        "    rephrase_df = prepare_text(rephrase_df, col='rephrase')\n",
        "    rephrase_df = rephrase_df[[\"rephrase\"]]\n",
        "\n",
        "    rephrase_df[\"sarcastic\"] = 0\n",
        "    rephrase_df.columns = [\"tweet\", \"sarcastic\"]\n",
        "    data = data[[\"tweet\", \"sarcastic\"]]\n",
        "    data = data.sample(frac=1).reset_index(drop=True)\n",
        "    df_train = data\n",
        "    df_train = pd.concat([df_train, rephrase_df])\n",
        "    df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    path = \"/content/drive/MyDrive/iSarcasm/Datasets2/split_80_20/val.csv\"\n",
        "    data = pd.read_csv(path, encoding='utf-8')\n",
        "    data = data[~data.tweet.isna()]\n",
        "    data['tweet'] = data['tweet'].apply(lambda x: preprocess(x))\n",
        "    data = prepare_text(data, col='tweet')\n",
        "    df_test = data\n",
        "    DF_train = TrainDataset(df_train, pretraine_path, max_length)\n",
        "    DF_test = TrainDataset(df_test, pretraine_path, max_length)\n",
        "\n",
        "    DF_train_loader = DataLoader(dataset=DF_train, batch_size=batchsize, shuffle=True,\n",
        "                                 num_workers=num_worker)\n",
        "    DF_test_loader = DataLoader(dataset=DF_test, batch_size=batchsize, shuffle=False,\n",
        "                                num_workers=num_worker)\n",
        "    return DF_train_loader, DF_test_loader"
      ],
      "metadata": {
        "id": "oa_4SMQ3jPsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTestData(batchsize=16, num_worker=2, pretraine_path=\"xlm-roberta-base\", max_length=128):\n",
        "    path = \"/content/drive/MyDrive/iSarcasm/Datasets2/tweet/task_A_Ar_test.csv\"\n",
        "    data = pd.read_csv(path, encoding='utf-8')\n",
        "    data['tweet'] = data['tweet'].apply(lambda x:preprocess(x))\n",
        "    data = prepare_text(data, col='tweet')\n",
        "\n",
        "    DF_test = TrainDataset(data, pretraine_path, max_length)\n",
        "\n",
        "    DF_test_loader = DataLoader(dataset=DF_test, batch_size=batchsize, shuffle=False,\n",
        "                                num_workers=num_worker)\n",
        "\n",
        "\n",
        "    return DF_test_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "-6tnueS6JUWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A8SY0Un6gbS"
      },
      "source": [
        "# Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr2rYqyv6i4F"
      },
      "outputs": [],
      "source": [
        "class FocalLoss_Ori(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
        "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
        "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
        "    Args:\n",
        "        num_class: number of classes\n",
        "        alpha: class balance factor\n",
        "        gamma:\n",
        "        ignore_index:\n",
        "        reduction:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_class, alpha=None, gamma=2, ignore_index=None, reduction='mean'):\n",
        "        super(FocalLoss_Ori, self).__init__()\n",
        "        self.num_class = num_class\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.smooth = 1e-4\n",
        "        self.ignore_index = ignore_index\n",
        "        self.alpha = alpha\n",
        "        if alpha is None:\n",
        "            self.alpha = torch.ones(num_class, )\n",
        "        elif isinstance(alpha, (int, float)):\n",
        "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
        "        elif isinstance(alpha, (list, np.ndarray)):\n",
        "            self.alpha = torch.as_tensor(alpha)\n",
        "        if self.alpha.shape[0] != num_class:\n",
        "            raise RuntimeError('the length not equal to number of class')\n",
        "\n",
        "    def forward(self, logit, target):\n",
        "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
        "        N, C = logit.shape[:2]\n",
        "        alpha = self.alpha.to(logit.device)\n",
        "        prob = F.softmax(logit, dim=1)\n",
        "        if prob.dim() > 2:\n",
        "            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
        "            prob = prob.view(N, C, -1)\n",
        "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
        "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
        "        ori_shp = target.shape\n",
        "        target = target.view(-1, 1)  # [N,d1,d2,...]->[N*d1*d2*...,1]\n",
        "        valid_mask = None\n",
        "        if self.ignore_index is not None:\n",
        "            valid_mask = target != self.ignore_index\n",
        "            target = target * valid_mask\n",
        "\n",
        "        # ----------memory saving way--------\n",
        "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
        "        logpt = torch.log(prob)\n",
        "        # alpha_class = alpha.gather(0, target.view(-1))\n",
        "        alpha_class = alpha[target.squeeze().long()]\n",
        "        class_weight = -alpha_class * torch.pow(torch.sub(1.0, prob), self.gamma)\n",
        "        loss = class_weight * logpt\n",
        "        if valid_mask is not None:\n",
        "            loss = loss * valid_mask.squeeze()\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "            if valid_mask is not None:\n",
        "                loss = loss.sum() / valid_mask.sum()\n",
        "\n",
        "        elif self.reduction == 'none':\n",
        "            loss = loss.view(ori_shp)\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4HVgjBd4eP"
      },
      "source": [
        "# modeling 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5UrAuWXd7yW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv2d') != -1 or classname.find('ConvTranspose2d') != -1:\n",
        "        nn.init.kaiming_uniform_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
        "        nn.init.zeros_(m.bias)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self,both=True,\n",
        "                pretrained_path='aubmindlab/bert-base-arabert'):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "\n",
        "        self.both = both\n",
        "        self.transformer = AutoModel.from_pretrained(pretrained_path, output_hidden_states=True)\n",
        "\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None):\n",
        "        outputs = self.transformer(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # (output_last_layer, pooled_cls, (output_layers))\n",
        "        # output[0] (8, seqlen=64, 768) cls [8, 768] ( 12 (8, seqlen=64, 768))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.transformer.config.hidden_size\n",
        "\n",
        "class ATTClassifier(nn.Module):\n",
        "    def __init__(self, in_feature, class_num=1, dropout_prob=0.2):\n",
        "        super(ATTClassifier, self).__init__()\n",
        "        self.model = GRU(input_size=in_feature, hidden_size=in_feature, num_layers=1, bias=True , output_size=in_feature)\n",
        "\n",
        "        self.Classifier = nn.Sequential(\n",
        "            nn.Linear(2 * in_feature, 512),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, class_num)\n",
        "        )\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        mod = self.model(x[0]) #(X[0] (bs, seqlenght, embedD) att = \\sum_i alpha_i x[0][i]\n",
        "        xx = torch.cat([mod, x[1]], 1)\n",
        "\n",
        "        out = self.Classifier(xx)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWb2TiUoQO-x"
      },
      "source": [
        "# train_sarcat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1-oxBZ-ZSfA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def train(base_model, mt_classifier, iterator, optimizer, sar_criterion, scheduler):\n",
        "\n",
        "    # set the model in eval phase\n",
        "    base_model.train(True)\n",
        "    mt_classifier.train(True)\n",
        "\n",
        "    acc_sarcasm= 0\n",
        "    f1_sarcasm =0\n",
        "    f1_score_macro = 0\n",
        "    loss_sarc= 0\n",
        "\n",
        "    all_sarcasm_outputs = np.array([])\n",
        "    all_sarcasm_labels = np.array([])\n",
        "\n",
        "    for data_input, label_input  in Bar(iterator):\n",
        "\n",
        "        for k, v in data_input.items():\n",
        "            data_input[k] = v.to(device)\n",
        "\n",
        "        for k, v in label_input.items():\n",
        "            label_input[k] = v.long().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        #forward pass\n",
        "\n",
        "        sarcasm_target = label_input['sarcasm']\n",
        "\n",
        "        # forward pass\n",
        "\n",
        "        output = base_model(**data_input)\n",
        "        sarcasm_logits = mt_classifier(output)\n",
        "\n",
        "        sarcasm_probs = torch.softmax(sarcasm_logits, dim=1)\n",
        "\n",
        "        loss_sarcasm = sar_criterion(sarcasm_logits, sarcasm_target)\n",
        "        loss_sarc += loss_sarcasm.item()\n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss_sarcasm.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        acc_sarcasm += calc_accuracy(sarcasm_probs, sarcasm_target)\n",
        "        _, predicted_sarcasm = torch.max(sarcasm_probs, 1)\n",
        "        all_sarcasm_outputs = np.append(all_sarcasm_outputs, predicted_sarcasm.squeeze().cpu().numpy())\n",
        "        all_sarcasm_labels = np.append(all_sarcasm_labels, sarcasm_target.squeeze().cpu().numpy())\n",
        "\n",
        "    all_sarcasm_outputs = all_sarcasm_outputs.reshape(-1)\n",
        "    all_sarcasm_labels = all_sarcasm_labels.reshape(-1)\n",
        "\n",
        "    fscore_macro = f1_score(y_true=all_sarcasm_labels, y_pred=all_sarcasm_outputs, average='macro')\n",
        "    fscore_sarcasm = f1_score(all_sarcasm_labels, all_sarcasm_outputs, average='binary', pos_label=1)\n",
        "\n",
        "\n",
        "    accuracies = { 'accuracy': acc_sarcasm / len(iterator), 'f1_sarcastic': fscore_sarcasm, \"f1_score\": fscore_macro}\n",
        "    losses = { 'loss': loss_sarc / len(iterator)}\n",
        "    return accuracies, losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFJSkOVlZW2y"
      },
      "outputs": [],
      "source": [
        "def evaluate(base_model, mt_classifier, iterator, sar_criterion):\n",
        "    # initialize every epoch\n",
        "    acc_sarcasm= 0\n",
        "    f1_sarcasm =0\n",
        "    f1_score_macro = 0\n",
        "    loss_sarc= 0\n",
        "\n",
        "    #all_sarcasm_outputs = []\n",
        "    all_sarcasm_outputs = np.array([])\n",
        "    all_sarcasm_labels = np.array([])\n",
        "\n",
        "    # set the model in eval phase\n",
        "    base_model.eval()\n",
        "    mt_classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        for data_input, label_input in Bar(iterator):\n",
        "\n",
        "            for k, v in data_input.items():\n",
        "                data_input[k] = v.to(device)\n",
        "\n",
        "            for k, v in label_input.items():\n",
        "                label_input[k] = v.long().to(device)\n",
        "\n",
        "\n",
        "            sarcasm_target = label_input['sarcasm']\n",
        "\n",
        "            # forward pass\n",
        "\n",
        "            output = base_model(**data_input)\n",
        "            sarcasm_logits = mt_classifier(output)\n",
        "            logits = sarcasm_logits[:,:2]\n",
        "\n",
        "            sarcasm_probs = torch.softmax(logits, dim=1)\n",
        "            # compute the loss\n",
        "            loss_sarcasm = sar_criterion(logits, sarcasm_target)\n",
        "\n",
        "            # compute the running accuracy and losses\n",
        "            acc_sarcasm += calc_accuracy(sarcasm_probs, sarcasm_target)\n",
        "            loss_sarc += loss_sarcasm.item()\n",
        "\n",
        "            _, predicted_sarcasm = torch.max(sarcasm_probs, 1)\n",
        "            #all_sarcasm_outputs.extend(predicted_sarcasm.squeeze().int().cpu().numpy().tolist())\n",
        "            all_sarcasm_outputs = np.append(all_sarcasm_outputs, predicted_sarcasm.squeeze().cpu().numpy())\n",
        "            #all_sarcasm_labels.extend(sarcasm_target.squeeze().int().cpu().numpy().tolist())\n",
        "            all_sarcasm_labels = np.append(all_sarcasm_labels, sarcasm_target.squeeze().cpu().numpy())\n",
        "    all_sarcasm_outputs = all_sarcasm_outputs.reshape(-1)\n",
        "    all_sarcasm_labels = all_sarcasm_labels.reshape(-1)\n",
        "\n",
        "    fscore_macro = f1_score(y_true=all_sarcasm_labels, y_pred=all_sarcasm_outputs, average='macro')\n",
        "    fscore_sarcasm = f1_score(all_sarcasm_labels, all_sarcasm_outputs, average='binary', pos_label=1)\n",
        "\n",
        "\n",
        "    report_sarcasm = classification_report(y_true=all_sarcasm_labels, y_pred=all_sarcasm_outputs,digits=4)\n",
        "\n",
        "\n",
        "    accuracies = { 'accuracy': acc_sarcasm / len(iterator), 'f1_sarcastic': fscore_sarcasm, \"f1_score\": fscore_macro, 'report_sarcasm': report_sarcasm}\n",
        "    losses = { 'loss': loss_sarc / len(iterator)}\n",
        "    return accuracies, losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf-lgC1LZcGu"
      },
      "outputs": [],
      "source": [
        "def predict(base_model, mt_classifier, iterator):\n",
        "    # initialize every epoch\n",
        "    acc_sarcasm= 0\n",
        "    loss_sarc= 0\n",
        "\n",
        "    f1_sarcasm =0\n",
        "    f1_score_macro = 0\n",
        "\n",
        "    #all_sarcasm_outputs = []\n",
        "    all_sarcasm_outputs = np.array([])\n",
        "    all_sarcasm_labels = np.array([])\n",
        "\n",
        "    # set the model in eval phase\n",
        "    base_model.eval()\n",
        "    mt_classifier.eval()\n",
        "    with torch.no_grad():\n",
        "      for data_input, label_input in Bar(iterator):\n",
        "            for k, v in data_input.items():\n",
        "                data_input[k] = v.to(device)\n",
        "\n",
        "            for k, v in label_input.items():\n",
        "                label_input[k] = v.long().to(device)\n",
        "\n",
        "            sarcasm_target = label_input['sarcasm']\n",
        "\n",
        "            # forward pass\n",
        "\n",
        "            output = base_model(**data_input)\n",
        "            sarcasm_logits = mt_classifier(output)\n",
        "            logits = sarcasm_logits[:,:2]\n",
        "            sarcasm_probs = torch.softmax(logits, dim=1)\n",
        "            # compute the loss\n",
        "            acc_sarcasm += calc_accuracy(sarcasm_probs, sarcasm_target)\n",
        "\n",
        "            _, predicted_sarcasm = torch.max(sarcasm_probs, 1)\n",
        "            #all_sarcasm_outputs.extend(predicted_sarcasm.squeeze().int().cpu().numpy().tolist())\n",
        "            all_sarcasm_outputs = np.append(all_sarcasm_outputs, predicted_sarcasm.squeeze().cpu().numpy())\n",
        "            all_sarcasm_labels = np.append(all_sarcasm_labels, sarcasm_target.squeeze().cpu().numpy())\n",
        "\n",
        "    all_sarcasm_outputs = all_sarcasm_outputs.reshape(-1)\n",
        "    all_sarcasm_labels = all_sarcasm_labels.reshape(-1)\n",
        "    fscore_macro = f1_score(y_true=all_sarcasm_labels, y_pred=all_sarcasm_outputs, average='macro')\n",
        "    fscore_sarcasm = f1_score(all_sarcasm_labels, all_sarcasm_outputs, average='binary', pos_label=1)\n",
        "\n",
        "\n",
        "    report_sarcasm = classification_report(y_true=all_sarcasm_labels, y_pred=all_sarcasm_outputs,digits=4)\n",
        "\n",
        "\n",
        "    accuracies = { 'accuracy': acc_sarcasm / len(iterator), 'f1_sarcastic': fscore_sarcasm, \"f1_score\": fscore_macro, 'report_sarcasm': report_sarcasm}\n",
        "    return accuracies, all_sarcasm_outputs, all_sarcasm_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_full(config, train_loader, stest_loader):\n",
        "    lr_o = config['lr_mult'] * config['lr']\n",
        "    lr = config['lr']\n",
        "    criterion = config['loss']\n",
        "    #Instanciate models\n",
        "    base_model = TransformerLayer(pretrained_path=config['pretrained_path'], both=True).to(device)\n",
        "    mtl_classifier = ATTClassifier(base_model.output_num(), class_num=2).to(device)\n",
        "    cls = 'ATTClassifier'\n",
        "\n",
        "    if criterion =='FL':\n",
        "        sarc_criterion = FocalLoss_Ori(num_class=2, alpha=[1, 1], gamma=2).to(device)\n",
        "    else:\n",
        "        sarc_criterion = nn.CrossEntropyLoss().to(device)\n",
        "    #\n",
        "\n",
        "    params = [{'params':base_model.parameters(), 'lr':config['lr']}, {'params': mtl_classifier.parameters(), 'lr': config['lr']}]#, {'params':multi_task_loss.parameters(), 'lr': 0.0005}]\n",
        "    optimizer = AdamW(params, lr=config[\"lr\"])\n",
        "\n",
        "    train_data_size = len(train_loader)\n",
        "    steps_per_epoch = int(train_data_size / config['batch_size'])\n",
        "    num_train_steps = len(train_loader) * config['epochs']\n",
        "    warmup_steps = int(config['epochs'] * train_data_size * 0.1 / config['batch_size'])\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,  num_warmup_steps=warmup_steps, num_training_steps=num_train_steps)\n",
        "    # Train model\n",
        "\n",
        "    best_val_accuracy = 0\n",
        "    best_val_f1_score = 0\n",
        "    best_val_f1_sarcastic = 0\n",
        "\n",
        "    best_val_metric = 0\n",
        "\n",
        "    best_val_loss = float('+inf')\n",
        "\n",
        "    best_report_sarcasm = None\n",
        "\n",
        "    epo = 0\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        # print(\"epoch {}\".format(epoch + 1))\n",
        "\n",
        "        train_accuracies, train_losses = train(base_model, mtl_classifier, train_loader, optimizer, sarc_criterion,scheduler)\n",
        "        valid_accuracies, valid_losses = evaluate(base_model, mtl_classifier, valid_loader, sarc_criterion)\n",
        "        val_loss = valid_losses['loss']\n",
        "        total_val_metric = valid_accuracies['f1_score']\n",
        "        if epoch == (config['epochs'] - 1):\n",
        "\n",
        "            epo = epoch+1\n",
        "            best_val_loss = val_loss\n",
        "            best_val_metric = total_val_metric\n",
        "\n",
        "            best_val_f1_score = valid_accuracies['f1_score']\n",
        "            best_val_f1_sarcastic = valid_accuracies['f1_sarcastic']\n",
        "            best_report_sarcasm = valid_accuracies['report_sarcasm']\n",
        "            best_val_accuracy= valid_accuracies['accuracy']\n",
        "            best_val_loss = valid_losses['loss']\n",
        "\n",
        "            print(\"save model's checkpoint\")\n",
        "            torch.save(base_model.state_dict(), f\"./ckpts/best_basemodel_sarcasm_{config['batch_size']}_{config['loss']}_{config['lr']}_ml{config['max_length']}_{config['epochs']}_{config['args']['seed']}_sarcat.pth\")\n",
        "            torch.save(mtl_classifier.state_dict(), f\"./ckpts/best_cls_sarcasm_{config['batch_size']}_{config['loss']}_{config['lr']}_ml{config['max_length']}_{config['epochs']}_{config['args']['seed']}_sarcat.pth\")\n",
        "\n",
        "    return best_val_accuracy, best_val_f1_score, best_val_f1_sarcastic, best_val_loss, epo"
      ],
      "metadata": {
        "id": "YDenIrPUIvrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBM3sBfzZl4c"
      },
      "outputs": [],
      "source": [
        "def eval_full(config, loader1):\n",
        "    criterion = config['loss']\n",
        "    base_model = TransformerLayer(pretrained_path=config['pretrained_path'], both=True).to(device)\n",
        "    classifier = ATTClassifier(base_model.output_num(), class_num=2).to(device)\n",
        "    base_model.load_state_dict(torch.load(f\"./ckpts/best_basemodel_sarcasm_{config['batch_size']}_{config['loss']}_{config['lr']}_ml{config['max_length']}_{config['epochs']}_{config['args']['seed']}_sarcat.pth\"))\n",
        "    classifier.load_state_dict(torch.load(f\"./ckpts/best_cls_sarcasm_{config['batch_size']}_{config['loss']}_{config['lr']}_ml{config['max_length']}_{config['epochs']}_{config['args']['seed']}_sarcat.pth\"))\n",
        "    base_model = base_model.to(device)\n",
        "    classifier = classifier.to(device)\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    accuracies, all_outputs_pred, all_outputs_label = predict(base_model, classifier, loader1)\n",
        "    return accuracies, all_outputs_pred, all_outputs_label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnYj2EAOZte_"
      },
      "outputs": [],
      "source": [
        "def plot_cf(cf_matrix):\n",
        "    ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues',fmt=\"d\")\n",
        "    ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
        "    ax.set_xlabel('\\nPredicted Values')\n",
        "    ax.set_ylabel('Actual Values ');\n",
        "    ## Ticket labels - List must be in alphabetical order\n",
        "    ax.xaxis.set_ticklabels(['False','True'])\n",
        "    ax.yaxis.set_ticklabels(['False','True'])\n",
        "    ## Display the visualization of the Confusion Matrix.\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "id": "pS3ZHef_3-if",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbad0fe-976d-49e3-cb9e-f01e6208c011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mckpts\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34mreports\u001b[0m/  \u001b[01;34mresults\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_arr = [1e-5,2e-5]\n",
        "batch_arr = [16,32,36,64]\n",
        "epoch_arr = [2,4]\n",
        "loss_arr=['FL']\n",
        "seed_arr=[298]"
      ],
      "metadata": {
        "id": "gHDHjV_d25sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/drive/MyDrive/iSarcasm/m_298'"
      ],
      "metadata": {
        "id": "B5evIk3J8FS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d23e65-b927-49aa-9a95-0cb1b331fd4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/iSarcasm/m_298’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# lr_arr = [2e-5]\n",
        "# batch_arr = [16]\n",
        "# epoch_arr = [2]\n",
        "# loss_arr=['FL']\n",
        "# seed_arr = [7774, 6911, 5751, 3723, 988, 2645, 3703, 2340, 298, 5423, 4759, 9054, 5495, 3120, 536, 16826, 3407 ]"
      ],
      "metadata": {
        "id": "J6Q7-A7FOMyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = ''\n",
        "best_f1 = 0"
      ],
      "metadata": {
        "id": "404wZAeH_eSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgfUDG6laoZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34710dd4-9205-4f3f-a45f-e656869f7b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_name 16_FL_1e-05_2_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.2s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.1s\n",
            "model_name 16_FL_1e-05_4_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.1s\n",
            "model_name 32_FL_1e-05_2_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.2s\n",
            "model_name 32_FL_1e-05_4_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.2s\n",
            "model_name 36_FL_1e-05_2_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.2s\n",
            "model_name 36_FL_1e-05_4_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.2s\n",
            "model_name 64_FL_1e-05_2_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.8s\n",
            "621/621: [============================>...] - ETA 0.3s\n",
            "3077/3077: [===============================>] - ETA 0.8s\n",
            "621/621: [============================>...] - ETA 0.2s\n",
            "save model's checkpoint\n",
            "1400/1400: [==============================>.] - ETA 0.3s\n",
            "model_name 64_FL_1e-05_4_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.8s\n",
            "621/621: [============================>...] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.7s\n",
            "621/621: [============================>...] - ETA 0.3s\n",
            "3077/3077: [===============================>] - ETA 0.7s\n",
            "621/621: [============================>...] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.8s\n",
            "621/621: [============================>...] - ETA 0.3s\n",
            "save model's checkpoint\n",
            "1400/1400: [==============================>.] - ETA 0.3s\n",
            "model_name 16_FL_2e-05_2_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.1s\n",
            "model_name 16_FL_2e-05_4_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.3s\n",
            "621/621: [===============================>] - ETA 0.1s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.1s\n",
            "model_name 32_FL_2e-05_2_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.2s\n",
            "model_name 32_FL_2e-05_4_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.2s\n",
            "model_name 36_FL_2e-05_2_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.2s\n",
            "model_name 36_FL_2e-05_4_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.4s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.1s\n",
            "3077/3077: [===============================>] - ETA 0.5s\n",
            "621/621: [==============================>.] - ETA 0.2s\n",
            "save model's checkpoint\n",
            "1400/1400: [===============================>] - ETA 0.2s\n",
            "model_name 64_FL_2e-05_2_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.8s\n",
            "621/621: [============================>...] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.8s\n",
            "621/621: [============================>...] - ETA 0.3s\n",
            "save model's checkpoint\n",
            "1400/1400: [==============================>.] - ETA 0.3s\n",
            "model_name 64_FL_2e-05_4_298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3077/3077: [===============================>] - ETA 0.8s\n",
            "621/621: [============================>...] - ETA 0.3s\n",
            "3077/3077: [===============================>] - ETA 0.8s\n",
            "621/621: [============================>...] - ETA 0.2s\n",
            "3077/3077: [===============================>] - ETA 0.7s\n",
            "621/621: [============================>...] - ETA 0.3s\n",
            "3077/3077: [===============================>] - ETA 0.7s\n",
            "621/621: [============================>...] - ETA 0.2s\n",
            "save model's checkpoint\n",
            "1400/1400: [==============================>.] - ETA 0.3s\n"
          ]
        }
      ],
      "source": [
        "for lr_i in lr_arr:\n",
        "  for batch_i in batch_arr:\n",
        "    for epoch_i in epoch_arr:\n",
        "      for loss_i in loss_arr:\n",
        "        for seed_i in seed_arr:\n",
        "\n",
        "            args = {}\n",
        "            args['lr_mult'] = 1.0\n",
        "            args['seed'] = seed_i\n",
        "            args['num_worker'] = 4\n",
        "            args['lang'] = 'ar'\n",
        "            args['phase'] = 'train'\n",
        "            args['lm_pretrained'] = 'marbert'\n",
        "\n",
        "            args['lr'] =lr_i\n",
        "            args['epochs'] = epoch_i\n",
        "            args['batch_size'] = batch_i\n",
        "            args['loss'] = loss_i\n",
        "            config = {}\n",
        "            config[\"max_length\"] = 64\n",
        "            config['args'] = args\n",
        "            config[\"output_for_test\"] = True\n",
        "            config['epochs'] = args['epochs']\n",
        "            config[\"class_num\"] = 1\n",
        "            config[\"lr\"] = args['lr']\n",
        "            config['lr_mult'] = args['lr_mult']\n",
        "            config['batch_size'] = args['batch_size']\n",
        "            config['lm'] = args['lm_pretrained']\n",
        "            config['loss'] = args['loss']\n",
        "            lang = args['lang']\n",
        "            dosegmentation = False\n",
        "\n",
        "\n",
        "            if args['lm_pretrained'] == 'marbert':\n",
        "                config['pretrained_path'] = \"UBC-NLP/MARBERT\"\n",
        "\n",
        "\n",
        "            RANDOM_SEED = config['args']['seed']\n",
        "\n",
        "            random.seed(RANDOM_SEED)\n",
        "            np.random.seed(RANDOM_SEED)\n",
        "            torch.manual_seed(RANDOM_SEED)\n",
        "            torch.cuda.manual_seed(RANDOM_SEED)\n",
        "            torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "            model_name = f\"{config['args']['batch_size']}_{config['args']['loss']}_{config['args']['lr']}_{config['args']['epochs']}_{config['args']['seed']}\"\n",
        "            print('model_name', model_name)\n",
        "            train_loader, valid_loader = loadTrainValData2(size = 0.2 ,batchsize=args['batch_size'], num_worker=0, pretraine_path=config['pretrained_path'], max_length=config['max_length'])\n",
        "            best_val_accuracy, best_val_f1_score, best_val_f1_sarcastic, best_val_loss, best_epo =train_full(config, train_loader, valid_loader)\n",
        "            if (best_val_f1_score > best_f1):\n",
        "              best_model = model_name\n",
        "              best_f1 = best_val_f1_score\n",
        "\n",
        "            results = {'model_name' : f'{model_name}'}\n",
        "            results['loss_fn']= config['args']['loss']\n",
        "            results['batch_size']=config['args']['batch_size']\n",
        "            results['lr']=config['args']['lr']\n",
        "            results['f1_sarcastic'] = best_val_f1_sarcastic\n",
        "            results['f1_score'] = best_val_f1_score\n",
        "            results['epoch']=config['args']['epochs']\n",
        "            results['seed']=config['args']['seed']\n",
        "\n",
        "\n",
        "            with open(results_h, 'a+') as f:\n",
        "                  f.write(json.dumps(results) + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "            args['phase'] = 'predict'\n",
        "            config['args'] = args\n",
        "\n",
        "            test_loader = loadTestData(batchsize=args['batch_size'], num_worker=0, pretraine_path=config['pretrained_path'],  max_length=config['max_length'])\n",
        "            test_accuracies, test_all_outputs, test_all_labels = eval_full(config, loader1=test_loader)\n",
        "\n",
        "            results = {'model_name' : f'{model_name}'}\n",
        "            results['loss_fn']= config['args']['loss']\n",
        "            results['batch_size']=config['args']['batch_size']\n",
        "            results['lr']=config['args']['lr']\n",
        "            results['f1_sarcastic'] = test_accuracies['f1_sarcastic']\n",
        "            results['f1_score'] = test_accuracies['f1_score']\n",
        "            results['epoch']=config['args']['epochs']\n",
        "            results['seed']=config['args']['seed']\n",
        "            with open(results_t, 'a+') as f:\n",
        "                  f.write(json.dumps(results) + '\\n')\n",
        "            base_file=f\"best_basemodel_sarcasm_{config['batch_size']}_{config['loss']}_{config['lr']}_ml{config['max_length']}_{config['epochs']}_{config['args']['seed']}_sarcat.pth\"\n",
        "            cls_file=f\"best_cls_sarcasm_{config['batch_size']}_{config['loss']}_{config['lr']}_ml{config['max_length']}_{config['epochs']}_{config['args']['seed']}_sarcat.pth\"\n",
        "\n",
        "            shutil.copy(f\"./ckpts/{base_file}\", f\"/content/drive/MyDrive/iSarcasm/m_298/{base_file}\")\n",
        "            shutil.copy(f\"./ckpts/{cls_file}\", f\"/content/drive/MyDrive/iSarcasm/m_298/{cls_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_db_h = pd.read_json(results_h, lines=True)\n",
        "print(\"results_db_h\", results_db_h)"
      ],
      "metadata": {
        "id": "psKTD5MklW7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5d0ea9-49d0-46c5-d26c-94a9328efd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results_db_h            model_name loss_fn  batch_size       lr  f1_sarcastic  f1_score  \\\n",
            "0   16_FL_1e-05_2_298      FL          16  0.00001      0.732824  0.830698   \n",
            "1   16_FL_1e-05_4_298      FL          16  0.00001      0.749035  0.841455   \n",
            "2   32_FL_1e-05_2_298      FL          32  0.00001      0.722222  0.825758   \n",
            "3   32_FL_1e-05_4_298      FL          32  0.00001      0.747082  0.840546   \n",
            "4   36_FL_1e-05_2_298      FL          36  0.00001      0.709163  0.817750   \n",
            "5   36_FL_1e-05_4_298      FL          36  0.00001      0.750000  0.841258   \n",
            "6   64_FL_1e-05_2_298      FL          64  0.00001      0.696356  0.810490   \n",
            "7   64_FL_1e-05_4_298      FL          64  0.00001      0.754864  0.845452   \n",
            "8   16_FL_2e-05_2_298      FL          16  0.00002      0.733591  0.831699   \n",
            "9   16_FL_2e-05_4_298      FL          16  0.00002      0.726562  0.827784   \n",
            "10  32_FL_2e-05_2_298      FL          32  0.00002      0.723735  0.825827   \n",
            "11  32_FL_2e-05_4_298      FL          32  0.00002      0.749035  0.841455   \n",
            "12  36_FL_2e-05_2_298      FL          36  0.00002      0.733591  0.831699   \n",
            "13  36_FL_2e-05_4_298      FL          36  0.00002      0.765799  0.850526   \n",
            "14  64_FL_2e-05_2_298      FL          64  0.00002      0.731518  0.830733   \n",
            "15  64_FL_2e-05_4_298      FL          64  0.00002      0.733591  0.831699   \n",
            "\n",
            "    epoch  seed  \n",
            "0       2   298  \n",
            "1       4   298  \n",
            "2       2   298  \n",
            "3       4   298  \n",
            "4       2   298  \n",
            "5       4   298  \n",
            "6       2   298  \n",
            "7       4   298  \n",
            "8       2   298  \n",
            "9       4   298  \n",
            "10      2   298  \n",
            "11      4   298  \n",
            "12      2   298  \n",
            "13      4   298  \n",
            "14      2   298  \n",
            "15      4   298  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_db_t = pd.read_json(results_t, lines=True)\n",
        "print(\"results_db_t\", results_db_t)"
      ],
      "metadata": {
        "id": "wL78BfKURMOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f161c69-44b9-4ef4-fe6c-b26c763a6fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results_db_t            model_name loss_fn  batch_size       lr  f1_sarcastic  f1_score  \\\n",
            "0   16_FL_1e-05_2_298      FL          16  0.00001      0.702439  0.825696   \n",
            "1   16_FL_1e-05_4_298      FL          16  0.00001      0.663529  0.801659   \n",
            "2   32_FL_1e-05_2_298      FL          32  0.00001      0.676617  0.811203   \n",
            "3   32_FL_1e-05_4_298      FL          32  0.00001      0.634259  0.783768   \n",
            "4   36_FL_1e-05_2_298      FL          36  0.00001      0.677249  0.813439   \n",
            "5   36_FL_1e-05_4_298      FL          36  0.00001      0.629550  0.777698   \n",
            "6   64_FL_1e-05_2_298      FL          64  0.00001      0.670330  0.810534   \n",
            "7   64_FL_1e-05_4_298      FL          64  0.00001      0.652681  0.794919   \n",
            "8   16_FL_2e-05_2_298      FL          16  0.00002      0.666667  0.803426   \n",
            "9   16_FL_2e-05_4_298      FL          16  0.00002      0.655963  0.796256   \n",
            "10  32_FL_2e-05_2_298      FL          32  0.00002      0.644231  0.791075   \n",
            "11  32_FL_2e-05_4_298      FL          32  0.00002      0.650000  0.792373   \n",
            "12  36_FL_2e-05_2_298      FL          36  0.00002      0.671679  0.808559   \n",
            "13  36_FL_2e-05_4_298      FL          36  0.00002      0.648526  0.791410   \n",
            "14  64_FL_2e-05_2_298      FL          64  0.00002      0.702703  0.826069   \n",
            "15  64_FL_2e-05_4_298      FL          64  0.00002      0.668182  0.803159   \n",
            "\n",
            "    epoch  seed  \n",
            "0       2   298  \n",
            "1       4   298  \n",
            "2       2   298  \n",
            "3       4   298  \n",
            "4       2   298  \n",
            "5       4   298  \n",
            "6       2   298  \n",
            "7       4   298  \n",
            "8       2   298  \n",
            "9       4   298  \n",
            "10      2   298  \n",
            "11      4   298  \n",
            "12      2   298  \n",
            "13      4   298  \n",
            "14      2   298  \n",
            "15      4   298  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/drive/MyDrive/iSarcasm/m_298/results'"
      ],
      "metadata": {
        "id": "xYKLemhDerFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"results_h.jsonl\", \"/content/drive/MyDrive/iSarcasm/m_298/results/results_h.jsonl\")\n",
        "shutil.copy(\"results_t.jsonl\", \"/content/drive/MyDrive/iSarcasm/m_298/results/results_t.jsonl\")\n",
        "# shutil.copy(\"best_model_f.jsonl\", \"/content/drive/MyDrive/iSarcasm/m_298/results/best_model_f.jsonl\")"
      ],
      "metadata": {
        "id": "ZXLV_a8QYZW_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8193b019-0015-4ab1-d9b2-0bce29f8bb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/iSarcasm/m_298/results/best_model_f.jsonl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import operator"
      ],
      "metadata": {
        "id": "eu5UPvVS6xQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_h = '/content/drive/MyDrive/iSarcasm/m_298/results/results_h.jsonl'\n",
        "results_t = '/content/drive/MyDrive/iSarcasm/m_298/results/results_t.jsonl'\n",
        "best_model_f = \"/content/drive/MyDrive/iSarcasm/m_298/results/best_model_f.jsonl\"\n",
        "\n",
        "if os.path.exists(best_model_f):\n",
        "    os.remove(best_model_f)\n",
        "\n",
        "results_db_h = pd.read_json(results_h, lines=True)\n",
        "results_db_t = pd.read_json(results_t, lines=True)\n",
        "\n",
        "individual_model_val_accs_h = {}\n",
        "for _, row in results_db_h.iterrows():\n",
        "    individual_model_val_accs_h[row['model_name']] = row['f1_score']\n",
        "\n",
        "individual_model_val_accs_h = sorted(individual_model_val_accs_h.items(), key=operator.itemgetter(1))\n",
        "individual_model_val_accs_h.reverse()\n",
        "sorted_models = [x[0] for x in individual_model_val_accs_h]\n",
        "\n",
        "\n",
        "\n",
        "for _, row in results_db_t.iterrows():\n",
        "    if row['model_name'] == sorted_models[0]:\n",
        "       row_t = row\n",
        "       break\n",
        "\n",
        "results = {'best_model' : f'{sorted_models[0]}'}\n",
        "\n",
        "results['loss_fn']= row_t['loss_fn']\n",
        "results['batch_size']=row_t['batch_size']\n",
        "results['lr']=row_t['lr']\n",
        "results['epoch']=row_t['epoch']\n",
        "results['seed']=row_t['seed']\n",
        "results['f1_sarcastic'] = row_t['f1_sarcastic']\n",
        "results['f1_score'] = row_t['f1_score']\n",
        "\n",
        "with open(best_model_f, 'a+') as f:\n",
        "         f.write(json.dumps(results) + '\\n')\n"
      ],
      "metadata": {
        "id": "qHoyDlknz1Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_db_t = pd.read_json(best_model_f, lines=True)\n",
        "print(\"best_model_db_t\", best_model_db_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Cs-O9f5jEg",
        "outputId": "7ebdfe59-faac-4191-9e3c-c1f7032ac62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model_db_t           best_model   loss_fn  batch_size       lr  epoch  seed  \\\n",
            "0  36_FL_2e-05_4_298  0.648526          36  0.00002      4   298   \n",
            "\n",
            "   f1_sarcastic  f1_score  \n",
            "0      0.648526   0.79141  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "guJiGSoy6_tY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}